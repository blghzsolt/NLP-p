# -*- coding: utf-8 -*-
"""f002kd_nlp_projekt

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10U0fsAIzD5vxw4sV77gGlZLvcet2bJtz

### üìö K√∂nyvkategoriz√°l√≥ NLP Projekt

**C√©l:** G√©pi tanul√°si modell fejleszt√©se, amely **le√≠r√°s √©s oldalsz√°m alapj√°n** automatikusan eld√∂nti, hogy egy k√∂nyv **gyerekeknek vagy feln≈ëtteknek** sz√≥l-e.

**A folyamat f≈ë l√©p√©sei:**
* Goodreads adatok let√∂lt√©se, c√≠mk√©k alapj√°n sz≈±r√©se √©s tiszt√≠t√°sa.
* **NLP & Feature Engineering:** Sz√∂vegelemz√©s, felesleges szavak t√∂rl√©se √©s TF-IDF vektoriz√°ci√≥.
* **Modellez√©s:** H√°rom algoritmus (Logisztikus Regresszi√≥, D√∂nt√©si Fa, Neur√°lis H√°l√≥) √∂sszehasonl√≠t√°sa a legpontosabb tal√°lat√©rt.
* **Deployment:** Egy interakt√≠v Gradio webes fel√ºlet, ahol a felhaszn√°l√≥k val√≥s id≈ëben tesztelhetik a v√©gs≈ë modellt szabadon v√°lasztott r√∂vid k√∂nyvismertet≈ë sz√∂vegekre.
"""

import gzip
import json
import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
import unittest
from collections import Counter
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
from scipy.sparse import hstack
import gradio as gr

# NLTK adatok let√∂lt√©se
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# FORR√ÅS: https://cseweb.ucsd.edu/~jmcauley/datasets/goodreads.html
# URL-ek defini√°l√°sa
URLS = {
    "child": ["https://mcauleylab.ucsd.edu/public_datasets/gdrive/goodreads/byGenre/goodreads_books_children.json.gz"],
    "adult": [
        "https://mcauleylab.ucsd.edu/public_datasets/gdrive/goodreads/byGenre/goodreads_books_history_biography.json.gz",
        "https://mcauleylab.ucsd.edu/public_datasets/gdrive/goodreads/byGenre/goodreads_books_mystery_thriller_crime.json.gz",
        "https://mcauleylab.ucsd.edu/public_datasets/gdrive/goodreads/byGenre/goodreads_books_romance.json.gz",
        "https://mcauleylab.ucsd.edu/public_datasets/gdrive/goodreads/byGenre/goodreads_books_fantasy_paranormal.json.gz"
    ]
}

# Glob√°lis v√°ltoz√≥k
all_books_data = []
seen_book_ids = set() # duplik√°ci√≥ sz≈±r√©shez

# ------ KATEG√ìRIA SZ≈∞R≈ê ------

def is_child_book(shelves):
    """Ellen≈ërzi: van 'children' 'polc' √âS 10-n√©l t√∂bben jel√∂lt√©k?"""
    if not shelves: return False
    for shelf in shelves:
        name = shelf.get('name', '').lower()
        count = int(shelf.get('count', 0))
        if 'children' in name and count > 10:
            return True
    return False

def is_adult_book(shelves):
    """Ellen≈ërzi: mentes minden gyerekre utal√≥ c√≠mk√©t≈ël?"""
    if not shelves: return True
    forbidden = ['children', 'young-adult']
    for shelf in shelves:
        name = shelf.get('name', '').lower()
        for tag in forbidden:
            if tag in name:
                return False # ha van gyerek c√≠mke, kiz√°rjuk
    return True

# ------ SPEC. LET√ñLT≈ê FV. ------

def process_url(url, target_label, limit=None):
    """URL let√∂lt√©se, sz≈±r√©se √©s hozz√°ad√°sa a k√∂z√∂s list√°hoz."""
    print(f"Feldolgoz√°s: {target_label} -> {url.split('/')[-1]}...")
    local_count = 0

    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with gzip.open(r.raw, mode='rt', encoding='utf-8') as f:
            for line in f:
                if limit and local_count >= limit: break

                try:
                    book = json.loads(line)
                    bid = book.get('book_id')

                    if bid in seen_book_ids: continue

                    shelves = book.get("popular_shelves")
                    match = False
                    if target_label == 'gyerek': match = is_child_book(shelves)
                    else: match = is_adult_book(shelves)

                    if match:
                        clean_data = {
                            'title': book.get('title_without_series'),
                            'description': book.get('description'),
                            'pages': book.get('num_pages'),
                            'label': target_label
                        }
                        all_books_data.append(clean_data)
                        seen_book_ids.add(bid)
                        local_count += 1

                except (json.JSONDecodeError, ValueError):
                    continue
    print(f"   -> K√©sz. Hozz√°adva: {local_count} db.")

# Gyerek k√∂nyvek gy≈±jt√©se
for url in URLS['child']:
    process_url(url, target_label='gyerek', limit=None)

# Feln≈ëtt k√∂nyvek gy≈±jt√©se
for url in URLS['adult']:
    process_url(url, target_label='feln≈ëtt', limit=10000)

print(f"\n√ñSSZES√çT√âS: {len(all_books_data)} db k√∂nyv sikeresen √∂sszegy≈±jtve.")

# ------ SZ√ñVEGTISZT√çT√ì ------

def preprocess_text(text):
    if not isinstance(text, str):
        return ""

    # kisbet≈±s√≠t√©s
    text = text.lower()

    # HTML tagek elt√°vol√≠t√°sa
    text = re.sub(r'<.*?>', '', text)

    # nem bet≈± karakterek (sz√°mok, √≠r√°sjelek) cser√©je sz√≥k√∂zre
    text = re.sub(r'[^a-z\s]', ' ', text)

    # felesleges sz√≥k√∂z√∂k elt√ºntet√©se
    text = re.sub(r'\s+', ' ', text).strip()

    return text

class TestProjectLogic(unittest.TestCase):

    # ------ SZ≈∞R≈êK TESZTEL√âSE ------
    def test_is_child_book(self):
        self.assertTrue(is_child_book([{'name': 'children-books', 'count': 50}]))

        # Hiba (count <= 10)
        self.assertFalse(is_child_book([{'name': 'children', 'count': 5}]))

    def test_is_adult_book(self):
        # Kiz√°r√≥ ok: young-adult
        self.assertFalse(is_adult_book([{'name': 'history', 'count': 100}, {'name': 'young-adult', 'count': 5}]))

        self.assertTrue(is_adult_book([{'name': 'crime', 'count': 20}]))

    # ------ TISZT√çT√ì F√úGGV√âNY TESZTEL√âSE ------
    def test_preprocess_text_html(self):
        raw = "<div>Hello <b>World</b></div>"
        expected = "hello world"
        self.assertEqual(preprocess_text(raw), expected)

    def test_preprocess_text_special_chars(self):
        raw = "Book #1: The Best of 2025!"
        expected = "book the best of"
        self.assertEqual(preprocess_text(raw), expected)

    def test_preprocess_text_empty(self):
        self.assertEqual(preprocess_text(None), "")
        self.assertEqual(preprocess_text(123), "")

print("------ UNIT TESZTEK ------")
unittest.main(argv=[''], verbosity=2, exit=False)

df = pd.DataFrame(all_books_data)
df_clean = df.copy()

print(f"Eredeti m√©ret: {len(df_clean)} sor")

# √ºres stringek NaN-ra
df_clean.replace(r'^\s*$', np.nan, regex=True, inplace=True)

# oldalsz√°m konvert√°l√°sa
df_clean['pages'] = pd.to_numeric(df_clean['pages'], errors='coerce')

# nincs c√≠m vagy le√≠r√°s, eldobjuk
df_clean = df_clean.dropna(subset=['title', 'description'])

# nincs oldalsz√°m, p√≥toljuk a medi√°nnal
median_pages = df_clean['pages'].median()
df_clean['pages'] = df_clean['pages'].fillna(median_pages).astype(int)

# t√∫l r√∂vid le√≠r√°sok t√∂rl√©se
df_clean = df_clean[df_clean['description'].str.len() > 20]

# (Gyerek=1, Feln≈ëtt=0)
label_map = {'feln≈ëtt': 0, 'gyerek': 1}
df_clean['label_numeric'] = df_clean['label'].map(label_map)

print(f"Tiszt√≠tott m√©ret: {len(df_clean)} sor (Eldobva: {len(df) - len(df_clean)})")
display(df_clean.head())

# ------ SZ√ñVEGTISZT√çT√ÅS ALKALMAZ√ÅSA ------

print("Sz√∂vegtiszt√≠t√°s folyamatban...")

df_clean['clean_text'] = df_clean['description'].apply(preprocess_text)

df_clean = df_clean[df_clean['clean_text'] != ""]

pd.set_option('display.max_colwidth', 50)
display(df_clean[['description', 'clean_text']].head(5))

# ------ VIZUALIZ√ÅCI√ìS ADATOK EL≈êK√âSZ√çT√âSE ------

print("Folyamatban...")

# Alap NLTK STOPWORDS
stop_words = set(nltk.corpus.stopwords.words('english'))

# SAJ√ÅT sz≈±r≈ëlista
custom_stops = {'book', 'story', 'read', 'one', 'life', 'new', 'time', 'world',
                'series', 'first', 'author', 'novel', 'characters', 'love'}
stop_words.update(custom_stops)

# LEAKAGE szavak
leakage_words = {
    'children', 'childrens', 'kids', 'child', 'kid', 'young', 'adult',
    'adults', 'teens', 'teen', 'teenager', 'readers', 'reader',
    'year', 'years', 'old', 'age', 'ages', 'grade'
}
stop_words.update(leakage_words)

def get_filtered_words(text_series):
    all_words = []
    for text in text_series:
        words = text.split()
        filtered = [w for w in words if w not in stop_words and len(w) > 2]
        all_words.extend(filtered)
    return all_words

# Gyerek k√∂nyvek szavai
child_texts = df_clean[df_clean['label'] == 'gyerek']['clean_text']
child_words = get_filtered_words(child_texts)

# Feln≈ëtt k√∂nyvek szavai
adult_texts = df_clean[df_clean['label'] == 'feln≈ëtt']['clean_text']
if len(adult_texts) > 20000:
    adult_texts = adult_texts.sample(n=20000, random_state=42)

adult_words = get_filtered_words(adult_texts)

print(f"Feldolgozott szavak sz√°ma: Gyerek={len(child_words)}, Feln≈ëtt={len(adult_words)}")

# --- WORDCLOUD √âS TOP SZAVAK ---

def plot_wordcloud(words_list, title):
    if not words_list: return
    text = " ".join(words_list)
    wc = WordCloud(width=800, height=400, background_color='white',
                   max_words=100, colormap='viridis', collocations=False, random_state=42).generate(text)

    plt.figure(figsize=(12, 6))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=18, pad=20)
    plt.show()

def plot_top_words(words_list, title, color):
    if not words_list: return
    counter = Counter(words_list)
    most_common = counter.most_common(15)

    words = [x[0] for x in most_common]
    counts = [x[1] for x in most_common]

    plt.figure(figsize=(10, 6))
    plt.barh(words, counts, color=color)
    plt.xlabel('El≈ëfordul√°s sz√°ma')
    plt.title(title, fontsize=14)
    plt.gca().invert_yaxis()
    plt.show()

df_clean['desc_length'] = df_clean['description'].str.len()

plt.figure(figsize=(6, 4))
sns.countplot(data=df_clean, x='label', palette='pastel', hue='label', legend=False)
plt.title("K√∂nyvek darabsz√°m eloszl√°sa")
plt.show()

plt.figure(figsize=(10, 5))
sns.histplot(data=df_clean, x='pages', hue='label', bins=50, palette={'gyerek': 'skyblue', 'feln≈ëtt': 'brown'})
plt.xlim(0, 800)
plt.title("Oldalsz√°mok eloszl√°sa (Gyerek vs Feln≈ëtt)")
plt.show()

print("\n--- Medi√°n √©rt√©kek ---")
stats = df_clean.groupby('label')[['pages', 'desc_length']].median()
display(stats)

plot_wordcloud(child_words, "Mir≈ël sz√≥lnak a GYEREKK√ñNYVEK?")
plot_top_words(child_words, "Top 15 sz√≥: Gyerekk√∂nyvek", 'skyblue')

plot_wordcloud(adult_words, "Mir≈ël sz√≥lnak a FELN≈êTT K√ñNYVEK?")
plot_top_words(adult_words, "Top 15 sz√≥: Feln≈ëtt k√∂nyvek", 'brown')

# ------ FEATURE ENGINEERING √âS ADATEL≈êK√âSZ√çT√âS ------

# stopwords lista a modellhez
final_stop_words = set(nltk.corpus.stopwords.words('english'))
final_stop_words.update(custom_stops)
final_stop_words.update(leakage_words)

# X (Jellemz≈ëk) el≈ë√°ll√≠t√°sa
# SZ√ñVEG: TF-IDF, csak az 5000 legfontosabb sz√≥val
tfidf = TfidfVectorizer(max_features=5000, stop_words=list(final_stop_words))
X_text = tfidf.fit_transform(df_clean['clean_text'])

# OLDALSZ√ÅM: Sk√°l√°z√°s, mert az oldalsz√°m sokkal nagyobb sz√°m, mint a TF-IDF √©rt√©kek
scaler = StandardScaler()
X_pages = scaler.fit_transform(df_clean[['pages']].values.astype(float))

# √ñSSZEF≈∞Z√âS
X = hstack([X_text, X_pages])
y = df_clean['label_numeric']

print(f"Bemeneti m√°trix m√©rete: {X.shape} (Sorok, Jellemz≈ëk)")

# adatok sz√©tv√°g√°sa (Train 70% / Validation / Test)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 15% valid√°ci√≥, 15% teszt
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print(f"\nTan√≠t√≥ halmaz (Train): {X_train.shape[0]} db")
print(f"Valid√°ci√≥s halmaz (Val): {X_val.shape[0]} db")
print(f"Teszt halmaz (Test):     {X_test.shape[0]} db")

# ------ MODELLEK TAN√çT√ÅSA ------

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(max_depth=20, random_state=42),
    "MLP Neural Net": MLPClassifier(hidden_layer_sizes=(50,), max_iter=200, verbose=True, early_stopping=True, random_state=42)
}

best_score = 0
best_model_name = ""
best_model_obj = None

print("Modellek versenyeztet√©se...")
print("-" * 40)

for name, model in models.items():
    print(f"Tr√©ning: {name}...")

    # tan√≠t√°s a TRAIN halmazon
    model.fit(X_train, y_train)

    # ellen≈ërz√©s a VALID√ÅCI√ìS halmazon
    y_pred_val = model.predict(X_val)
    acc = accuracy_score(y_val, y_pred_val)

    print(f"  -> Pontoss√°g (Accuracy): {acc:.4f} ({acc*100:.2f}%)")

    # Legjobb
    if acc > best_score:
        best_score = acc
        best_model_name = name
        best_model_obj = model

print("-" * 40)
print(f"GY≈êZTES MODELL: {best_model_name} ({best_score:.2f}% pontoss√°ggal)")

# ------ Feature Importance ------

# logisztikus regresszi√≥ eset√©n
ref_model = models["Logistic Regression"]

# szavakat a TF-IDF-b≈ël
feature_names = tfidf.get_feature_names_out()

# oldalsz√°m
feature_names = np.append(feature_names, "OLDALSZ√ÅM (PAGES)")

# s√∫lyok
coefs = ref_model.coef_[0]

df_imp = pd.DataFrame({'feature': feature_names, 'importance': coefs})
df_imp = df_imp.sort_values(by='importance', ascending=False)

print("------ A MODELL SZERINTI KULCSSZAVAK ------")
print("\nTOP 10 'GYEREK' JELLEMZ≈ê (Pozit√≠v s√∫lyok):")
print(df_imp.head(10))

print("\nTOP 10 'FELN≈êTT' JELLEMZ≈ê (Negat√≠v s√∫lyok):")
print(df_imp.tail(10))

plt.figure(figsize=(10, 6))
top_bottom = pd.concat([df_imp.head(10), df_imp.tail(10)])
colors = ['green' if x > 0 else 'red' for x in top_bottom['importance']]
plt.barh(top_bottom['feature'], top_bottom['importance'], color=colors)
plt.title("Mely szavak d√∂ntenek legink√°bb? (Z√∂ld=Gyerek, Piros=Feln≈ëtt)")
plt.xlabel("S√∫ly (Fontoss√°g)")
plt.show()

# ------ MODELL √âRT√âKEL√âSE A TESZT HALMAZON ------

print(f"V√©gs≈ë teszt futtat√°sa a '{best_model_name}' modellel...")

# el≈ërejelz√©s a TESZT halmazon
y_pred_test = best_model_obj.predict(X_test)

# val√≥sz√≠n≈±s√©gek a ROC g√∂rb√©hez
if hasattr(best_model_obj, "predict_proba"):
    y_prob_test = best_model_obj.predict_proba(X_test)[:, 1]
else:
    y_prob_test = y_pred_test

# ------ KONF√öZI√ìS M√ÅTRIX (t√©ved√©sek) ------
cm = confusion_matrix(y_test, y_pred_test)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Feln≈ëtt', 'Gyerek'])

fig, ax = plt.subplots(figsize=(6, 6))
disp.plot(cmap='Blues', ax=ax)
plt.title(f'T√©veszt√©si M√°trix\n(Modell: {best_model_name})')
plt.grid(False)
plt.show()

# ------ ROC G√ñRBE (teljes√≠tm√©ny g√∂rbe) ------
fpr, tpr, thresholds = roc_curve(y_test, y_prob_test)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Modell teljes√≠tm√©nye (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='V√©letlen tippel√©s')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (T√©ves riaszt√°sok)')
plt.ylabel('True Positive Rate (Val√≥s tal√°latok)')
plt.title('ROC G√∂rbe')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()

print(classification_report(y_test, y_pred_test, target_names=['Feln≈ëtt', 'Gyerek']))

# ------ GRADIO alkalmaz√°s ------

def predict_book_category(description, num_pages):

    clean_desc = preprocess_text(description)
    if not clean_desc or len(clean_desc) < 5:
        return {"Hiba: T√∫l r√∂vid vagy √©rtelmezhetetlen sz√∂veg": 1.0}

    # VEKTORIZ√ÅL√ÅS (A betan√≠tott 'tfidf' objektummal)
    text_vector = tfidf.transform([clean_desc])

    # OLDALSZ√ÅM SK√ÅL√ÅZ√ÅS (A betan√≠tott 'scaler' objektummal)
    pages_scaled = scaler.transform([[num_pages]])

    # √ñSSZEF≈∞Z√âS
    features = hstack([text_vector, pages_scaled])

    # PREDIKCI√ì
    probs = best_model_obj.predict_proba(features)[0]

    return {
        "Feln≈ëtteknek sz√≥l√≥ k√∂nyv üîû": float(probs[0]),
        "Gyerekk√∂nyv üß∏": float(probs[1])
    }

demo = gr.Interface(
    fn=predict_book_category,
    inputs=[
        gr.Textbox(lines=5, placeholder="Ide m√°sold a k√∂nyv angol le√≠r√°s√°t...", label="Description"),
        gr.Number(value=250, label="Oldalak sz√°ma")
    ],
    outputs=gr.Label(num_top_classes=2, label="Az AI d√∂nt√©se"),
    title="Bin√°ris üìö oszt√°lyoz√≥",
    description="A modell le√≠r√°s √©s az oldalsz√°m alapj√°n d√∂nti el, hogy egy k√∂nyv gyerekeknek vagy feln≈ëtteknek sz√≥l-e.",
    examples=[
        ["The classic work on investing, filled with sound and safe principles that are as reliable as ever, now revised with an introduction and appendix by financial legend Warren Buffett‚Äîone of the author‚Äôs most famous students‚Äîand newly updated commentaries on each chapter from distinguished Wall Street Journal writer Jason Zweig.", 640],
        ["Winnie the Pooh has enchanted readers of all ages for nearly one hundred years with its relatable, heartwarming adventures that follow the famously friendly and lovable teddy bear. In this classic collection, Winnie navigates the Hundred Acre Wood with Christopher Robin, Eeyore, Piglet, Owl, and Rabbit, learning the true meaning of friendship and the value of accepting everyone exactly as they are.", 176],
        ["A detective investigates a mysterious murder in a small town where everyone has secrets.", 350]
    ],
    theme="soft",
    allow_flagging="never"
)

print("Az alkalmaz√°s ind√≠t√°sa...")
demo.launch(share=True, debug=True)

"""### √ñsszegz√©s √©s eredm√©nyek

A projekt c√©lja egy olyan k√∂nyvkategoriz√°l√≥ modell l√©trehoz√°sa volt, amely k√©pes megk√ºl√∂nb√∂ztetni a gyermek √©s a feln≈ëttirodalmat.

A jelenlegi megold√°s **93% feletti pontoss√°got** √©r el, ami √∂nmag√°ban j√≥ eredm√©nynek sz√°m√≠t. Fontos azonban hangs√∫lyozni, hogy a modell egy leegyszer≈±s√≠tett, k√©toszt√°lyos feladatra k√©sz√ºlt, ami jelent≈ësen megk√∂nny√≠ti a d√∂nt√©si folyamatot.

### Korl√°tok

* **Nyelvi k√∂t√∂tts√©g:** A rendszer kiz√°r√≥lag **angol nyelv≈±** sz√∂vegeken alkalmazhat√≥, mivel a haszn√°lt sz√≥t√°rak √©s nyelvi er≈ëforr√°sok erre a nyelvre √©p√ºlnek.
* **Merev kateg√≥riarendszer:** A modell csak k√©t kateg√≥ri√°val dolgozik (gyermek vs. feln≈ëtt), ez√©rt az √°tmeneti m≈±fajokat (pl. az ifj√∫s√°gi / Young Adult k√∂nyveket) nem k√©pes √∂n√°ll√≥an kezelni, hanem sz√ºks√©gszer≈±en az egyik csoportba sorolja ≈ëket.
* Jelenleg a d√∂nt√©sek els≈ësorban kulcsszavak jelenl√©t√©n alapulnak, √≠gy a modell nem √©rti a sz√∂veg m√©lyebb jelent√©s√©t, kontextus√°t vagy az olyan nyelvi jelens√©geket, mint az ir√≥nia.

### Fejleszt√©si lehet≈ës√©gek

* **Kontextusalap√∫ nyelvi modellek alkalmaz√°sa:** Az alkalmazott statisztikai megk√∂zel√≠t√©s tov√°bbfejleszthet≈ë modern nyelvi modellekkel (p√©ld√°ul **BERT**), amelyek k√©pesek figyelembe venni a mondatok szerkezet√©t √©s a sz√∂vegk√∂rnyezetet.
* **K√©pi inform√°ci√≥k bevon√°sa:** A sz√∂veges elemz√©st ki lehetne eg√©sz√≠teni a **k√∂nyvbor√≠t√≥k vizu√°lis feldolgoz√°s√°val**, mivel a gyermekk√∂nyvek grafikai vil√°ga gyakran j√≥l elk√ºl√∂n√≠thet≈ë a feln≈ëtteknek sz√°nt kiadv√°nyok√©t√≥l.
"""